{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kaRrkF0BSMA3",
        "outputId": "cda13e89-134a-4192-d6ec-88675a361fab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pdf2image\n",
            "  Downloading pdf2image-1.17.0-py3-none-any.whl.metadata (6.2 kB)\n",
            "Requirement already satisfied: pillow in c:\\users\\devba\\miniconda3\\lib\\site-packages (from pdf2image) (10.0.1)\n",
            "Downloading pdf2image-1.17.0-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: pdf2image\n",
            "Successfully installed pdf2image-1.17.0\n"
          ]
        }
      ],
      "source": [
        "!pip install pdf2image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FGg2a8gZTeep",
        "outputId": "21f12283-daff-4e5c-e6f5-e64ebdf55b16"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Channels:\n",
            " - defaults\n",
            " - conda-forge\n",
            " - nvidia\n",
            " - pytorch\n",
            "Platform: win-64\n",
            "Collecting package metadata (repodata.json): ...working... done\n",
            "Solving environment: ...working... failed\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "PackagesNotFoundError: The following packages are not available from current channels:\n",
            "\n",
            "  - tesseract-ocr\n",
            "\n",
            "Current channels:\n",
            "\n",
            "  - defaults\n",
            "  - https://conda.anaconda.org/conda-forge/noarch\n",
            "  - https://conda.anaconda.org/nvidia/win-64\n",
            "  - https://conda.anaconda.org/conda-forge/win-64\n",
            "  - https://conda.anaconda.org/pytorch/win-64\n",
            "  - https://conda.anaconda.org/pytorch/noarch\n",
            "\n",
            "To search for alternate channels that may provide the conda package you're\n",
            "looking for, navigate to\n",
            "\n",
            "    https://anaconda.org\n",
            "\n",
            "and use the search bar at the top of the page.\n",
            "\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pytesseract in c:\\users\\devba\\miniconda3\\lib\\site-packages (0.3.10)\n",
            "Requirement already satisfied: packaging>=21.3 in c:\\users\\devba\\miniconda3\\lib\\site-packages (from pytesseract) (23.1)\n",
            "Requirement already satisfied: Pillow>=8.0.0 in c:\\users\\devba\\miniconda3\\lib\\site-packages (from pytesseract) (10.0.1)\n"
          ]
        }
      ],
      "source": [
        "!conda install tesseract-ocr\n",
        "\n",
        "!pip install pytesseract"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uso38R2QShxC",
        "outputId": "2a302b0f-c443-4edc-fe80-00e6ddb2b17e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Channels:\n",
            " - defaults\n",
            " - conda-forge\n",
            " - nvidia\n",
            " - pytorch\n",
            "Platform: win-64\n",
            "Collecting package metadata (repodata.json): ...working... done\n",
            "Solving environment: ...working... failed\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "PackagesNotFoundError: The following packages are not available from current channels:\n",
            "\n",
            "  - poppler-utils\n",
            "\n",
            "Current channels:\n",
            "\n",
            "  - defaults\n",
            "  - https://conda.anaconda.org/conda-forge/noarch\n",
            "  - https://conda.anaconda.org/nvidia/win-64\n",
            "  - https://conda.anaconda.org/conda-forge/win-64\n",
            "  - https://conda.anaconda.org/pytorch/win-64\n",
            "  - https://conda.anaconda.org/pytorch/noarch\n",
            "\n",
            "To search for alternate channels that may provide the conda package you're\n",
            "looking for, navigate to\n",
            "\n",
            "    https://anaconda.org\n",
            "\n",
            "and use the search bar at the top of the page.\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!conda install poppler-utils\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PawdGlo6TKx_",
        "outputId": "65309f7b-b9d7-4a16-8a1a-145f7e38c57d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting tabula-py\n",
            "  Downloading tabula_py-2.9.0-py3-none-any.whl.metadata (7.5 kB)\n",
            "Requirement already satisfied: pandas>=0.25.3 in c:\\users\\devba\\miniconda3\\lib\\site-packages (from tabula-py) (2.1.4)\n",
            "Requirement already satisfied: numpy in c:\\users\\devba\\miniconda3\\lib\\site-packages (from tabula-py) (1.24.3)\n",
            "Requirement already satisfied: distro in c:\\users\\devba\\miniconda3\\lib\\site-packages (from tabula-py) (1.8.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\devba\\miniconda3\\lib\\site-packages (from pandas>=0.25.3->tabula-py) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\devba\\miniconda3\\lib\\site-packages (from pandas>=0.25.3->tabula-py) (2023.3.post1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\devba\\miniconda3\\lib\\site-packages (from pandas>=0.25.3->tabula-py) (2023.3)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\devba\\miniconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas>=0.25.3->tabula-py) (1.16.0)\n",
            "Downloading tabula_py-2.9.0-py3-none-any.whl (12.0 MB)\n",
            "   ---------------------------------------- 0.0/12.0 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/12.0 MB 1.4 MB/s eta 0:00:09\n",
            "   ---------------------------------------- 0.1/12.0 MB 991.0 kB/s eta 0:00:13\n",
            "   ---------------------------------------- 0.1/12.0 MB 1.2 MB/s eta 0:00:10\n",
            "   ---------------------------------------- 0.1/12.0 MB 853.3 kB/s eta 0:00:14\n",
            "    --------------------------------------- 0.3/12.0 MB 1.1 MB/s eta 0:00:11\n",
            "    --------------------------------------- 0.3/12.0 MB 1.1 MB/s eta 0:00:11\n",
            "   - -------------------------------------- 0.4/12.0 MB 1.2 MB/s eta 0:00:10\n",
            "   - -------------------------------------- 0.4/12.0 MB 1.2 MB/s eta 0:00:10\n",
            "   - -------------------------------------- 0.5/12.0 MB 1.2 MB/s eta 0:00:10\n",
            "   - -------------------------------------- 0.5/12.0 MB 1.2 MB/s eta 0:00:10\n",
            "   - -------------------------------------- 0.6/12.0 MB 1.2 MB/s eta 0:00:10\n",
            "   -- ------------------------------------- 0.7/12.0 MB 1.3 MB/s eta 0:00:10\n",
            "   -- ------------------------------------- 0.7/12.0 MB 1.3 MB/s eta 0:00:09\n",
            "   -- ------------------------------------- 0.8/12.0 MB 1.3 MB/s eta 0:00:09\n",
            "   -- ------------------------------------- 0.9/12.0 MB 1.3 MB/s eta 0:00:09\n",
            "   --- ------------------------------------ 1.0/12.0 MB 1.4 MB/s eta 0:00:08\n",
            "   --- ------------------------------------ 1.1/12.0 MB 1.4 MB/s eta 0:00:08\n",
            "   --- ------------------------------------ 1.1/12.0 MB 1.4 MB/s eta 0:00:08\n",
            "   --- ------------------------------------ 1.2/12.0 MB 1.4 MB/s eta 0:00:08\n",
            "   ---- ----------------------------------- 1.3/12.0 MB 1.4 MB/s eta 0:00:08\n",
            "   ---- ----------------------------------- 1.4/12.0 MB 1.4 MB/s eta 0:00:08\n",
            "   ---- ----------------------------------- 1.4/12.0 MB 1.4 MB/s eta 0:00:08\n",
            "   ----- ---------------------------------- 1.5/12.0 MB 1.5 MB/s eta 0:00:08\n",
            "   ----- ---------------------------------- 1.6/12.0 MB 1.4 MB/s eta 0:00:08\n",
            "   ----- ---------------------------------- 1.7/12.0 MB 1.5 MB/s eta 0:00:08\n",
            "   ----- ---------------------------------- 1.7/12.0 MB 1.4 MB/s eta 0:00:08\n",
            "   ------ --------------------------------- 1.8/12.0 MB 1.5 MB/s eta 0:00:07\n",
            "   ------ --------------------------------- 1.9/12.0 MB 1.5 MB/s eta 0:00:07\n",
            "   ------ --------------------------------- 1.9/12.0 MB 1.5 MB/s eta 0:00:07\n",
            "   ------ --------------------------------- 2.0/12.0 MB 1.5 MB/s eta 0:00:07\n",
            "   ------ --------------------------------- 2.1/12.0 MB 1.5 MB/s eta 0:00:07\n",
            "   ------ --------------------------------- 2.1/12.0 MB 1.4 MB/s eta 0:00:07\n",
            "   ------- -------------------------------- 2.2/12.0 MB 1.4 MB/s eta 0:00:07\n",
            "   ------- -------------------------------- 2.2/12.0 MB 1.4 MB/s eta 0:00:07\n",
            "   ------- -------------------------------- 2.3/12.0 MB 1.4 MB/s eta 0:00:07\n",
            "   ------- -------------------------------- 2.3/12.0 MB 1.4 MB/s eta 0:00:07\n",
            "   -------- ------------------------------- 2.4/12.0 MB 1.4 MB/s eta 0:00:07\n",
            "   -------- ------------------------------- 2.5/12.0 MB 1.4 MB/s eta 0:00:07\n",
            "   -------- ------------------------------- 2.6/12.0 MB 1.4 MB/s eta 0:00:07\n",
            "   -------- ------------------------------- 2.7/12.0 MB 1.4 MB/s eta 0:00:07\n",
            "   --------- ------------------------------ 2.7/12.0 MB 1.4 MB/s eta 0:00:07\n",
            "   --------- ------------------------------ 2.8/12.0 MB 1.5 MB/s eta 0:00:07\n",
            "   --------- ------------------------------ 2.9/12.0 MB 1.5 MB/s eta 0:00:07\n",
            "   --------- ------------------------------ 2.9/12.0 MB 1.4 MB/s eta 0:00:07\n",
            "   ---------- ----------------------------- 3.0/12.0 MB 1.5 MB/s eta 0:00:07\n",
            "   ---------- ----------------------------- 3.1/12.0 MB 1.5 MB/s eta 0:00:07\n",
            "   ---------- ----------------------------- 3.2/12.0 MB 1.5 MB/s eta 0:00:07\n",
            "   ---------- ----------------------------- 3.2/12.0 MB 1.5 MB/s eta 0:00:06\n",
            "   ----------- ---------------------------- 3.3/12.0 MB 1.5 MB/s eta 0:00:06\n",
            "   ----------- ---------------------------- 3.4/12.0 MB 1.5 MB/s eta 0:00:06\n",
            "   ----------- ---------------------------- 3.5/12.0 MB 1.5 MB/s eta 0:00:06\n",
            "   ----------- ---------------------------- 3.6/12.0 MB 1.5 MB/s eta 0:00:06\n",
            "   ------------ --------------------------- 3.7/12.0 MB 1.5 MB/s eta 0:00:06\n",
            "   ------------ --------------------------- 3.7/12.0 MB 1.5 MB/s eta 0:00:06\n",
            "   ------------ --------------------------- 3.8/12.0 MB 1.5 MB/s eta 0:00:06\n",
            "   ------------ --------------------------- 3.9/12.0 MB 1.5 MB/s eta 0:00:06\n",
            "   ------------- -------------------------- 4.0/12.0 MB 1.5 MB/s eta 0:00:06\n",
            "   ------------- -------------------------- 4.0/12.0 MB 1.5 MB/s eta 0:00:06\n",
            "   ------------- -------------------------- 4.1/12.0 MB 1.5 MB/s eta 0:00:06\n",
            "   ------------- -------------------------- 4.2/12.0 MB 1.5 MB/s eta 0:00:06\n",
            "   -------------- ------------------------- 4.2/12.0 MB 1.5 MB/s eta 0:00:06\n",
            "   -------------- ------------------------- 4.3/12.0 MB 1.5 MB/s eta 0:00:06\n",
            "   -------------- ------------------------- 4.4/12.0 MB 1.5 MB/s eta 0:00:06\n",
            "   -------------- ------------------------- 4.5/12.0 MB 1.5 MB/s eta 0:00:06\n",
            "   --------------- ------------------------ 4.5/12.0 MB 1.5 MB/s eta 0:00:05\n",
            "   --------------- ------------------------ 4.6/12.0 MB 1.5 MB/s eta 0:00:05\n",
            "   --------------- ------------------------ 4.7/12.0 MB 1.5 MB/s eta 0:00:05\n",
            "   --------------- ------------------------ 4.7/12.0 MB 1.5 MB/s eta 0:00:05\n",
            "   ---------------- ----------------------- 4.8/12.0 MB 1.5 MB/s eta 0:00:05\n",
            "   ---------------- ----------------------- 4.9/12.0 MB 1.5 MB/s eta 0:00:05\n",
            "   ---------------- ----------------------- 5.0/12.0 MB 1.5 MB/s eta 0:00:05\n",
            "   ---------------- ----------------------- 5.1/12.0 MB 1.5 MB/s eta 0:00:05\n",
            "   ---------------- ----------------------- 5.1/12.0 MB 1.5 MB/s eta 0:00:05\n",
            "   ----------------- ---------------------- 5.1/12.0 MB 1.5 MB/s eta 0:00:05\n",
            "   ----------------- ---------------------- 5.2/12.0 MB 1.5 MB/s eta 0:00:05\n",
            "   ----------------- ---------------------- 5.3/12.0 MB 1.5 MB/s eta 0:00:05\n",
            "   ----------------- ---------------------- 5.3/12.0 MB 1.5 MB/s eta 0:00:05\n",
            "   ----------------- ---------------------- 5.4/12.0 MB 1.5 MB/s eta 0:00:05\n",
            "   ------------------ --------------------- 5.4/12.0 MB 1.5 MB/s eta 0:00:05\n",
            "   ------------------ --------------------- 5.5/12.0 MB 1.5 MB/s eta 0:00:05\n",
            "   ------------------ --------------------- 5.6/12.0 MB 1.5 MB/s eta 0:00:05\n",
            "   ------------------ --------------------- 5.7/12.0 MB 1.5 MB/s eta 0:00:05\n",
            "   ------------------- -------------------- 5.7/12.0 MB 1.5 MB/s eta 0:00:05\n",
            "   ------------------- -------------------- 5.8/12.0 MB 1.5 MB/s eta 0:00:05\n",
            "   ------------------- -------------------- 5.9/12.0 MB 1.5 MB/s eta 0:00:05\n",
            "   ------------------- -------------------- 5.9/12.0 MB 1.5 MB/s eta 0:00:05\n",
            "   ------------------- -------------------- 6.0/12.0 MB 1.5 MB/s eta 0:00:05\n",
            "   -------------------- ------------------- 6.1/12.0 MB 1.5 MB/s eta 0:00:05\n",
            "   -------------------- ------------------- 6.1/12.0 MB 1.5 MB/s eta 0:00:04\n",
            "   -------------------- ------------------- 6.2/12.0 MB 1.5 MB/s eta 0:00:04\n",
            "   -------------------- ------------------- 6.2/12.0 MB 1.5 MB/s eta 0:00:04\n",
            "   -------------------- ------------------- 6.3/12.0 MB 1.5 MB/s eta 0:00:04\n",
            "   --------------------- ------------------ 6.3/12.0 MB 1.5 MB/s eta 0:00:04\n",
            "   --------------------- ------------------ 6.4/12.0 MB 1.5 MB/s eta 0:00:04\n",
            "   --------------------- ------------------ 6.5/12.0 MB 1.5 MB/s eta 0:00:04\n",
            "   --------------------- ------------------ 6.5/12.0 MB 1.5 MB/s eta 0:00:04\n",
            "   --------------------- ------------------ 6.5/12.0 MB 1.5 MB/s eta 0:00:04\n",
            "   --------------------- ------------------ 6.6/12.0 MB 1.5 MB/s eta 0:00:04\n",
            "   ---------------------- ----------------- 6.6/12.0 MB 1.4 MB/s eta 0:00:04\n",
            "   ---------------------- ----------------- 6.7/12.0 MB 1.4 MB/s eta 0:00:04\n",
            "   ---------------------- ----------------- 6.8/12.0 MB 1.4 MB/s eta 0:00:04\n",
            "   ---------------------- ----------------- 6.8/12.0 MB 1.4 MB/s eta 0:00:04\n",
            "   ----------------------- ---------------- 6.9/12.0 MB 1.5 MB/s eta 0:00:04\n",
            "   ----------------------- ---------------- 7.0/12.0 MB 1.4 MB/s eta 0:00:04\n",
            "   ----------------------- ---------------- 7.0/12.0 MB 1.4 MB/s eta 0:00:04\n",
            "   ----------------------- ---------------- 7.1/12.0 MB 1.4 MB/s eta 0:00:04\n",
            "   ----------------------- ---------------- 7.2/12.0 MB 1.5 MB/s eta 0:00:04\n",
            "   ------------------------ --------------- 7.2/12.0 MB 1.4 MB/s eta 0:00:04\n",
            "   ------------------------ --------------- 7.3/12.0 MB 1.4 MB/s eta 0:00:04\n",
            "   ------------------------ --------------- 7.4/12.0 MB 1.4 MB/s eta 0:00:04\n",
            "   ------------------------ --------------- 7.4/12.0 MB 1.4 MB/s eta 0:00:04\n",
            "   ------------------------ --------------- 7.5/12.0 MB 1.4 MB/s eta 0:00:04\n",
            "   ------------------------- -------------- 7.5/12.0 MB 1.4 MB/s eta 0:00:04\n",
            "   ------------------------- -------------- 7.6/12.0 MB 1.4 MB/s eta 0:00:04\n",
            "   ------------------------- -------------- 7.6/12.0 MB 1.4 MB/s eta 0:00:04\n",
            "   ------------------------- -------------- 7.6/12.0 MB 1.4 MB/s eta 0:00:04\n",
            "   ------------------------- -------------- 7.7/12.0 MB 1.4 MB/s eta 0:00:04\n",
            "   ------------------------- -------------- 7.8/12.0 MB 1.4 MB/s eta 0:00:03\n",
            "   -------------------------- ------------- 7.8/12.0 MB 1.4 MB/s eta 0:00:03\n",
            "   -------------------------- ------------- 7.9/12.0 MB 1.4 MB/s eta 0:00:03\n",
            "   -------------------------- ------------- 8.0/12.0 MB 1.4 MB/s eta 0:00:03\n",
            "   -------------------------- ------------- 8.1/12.0 MB 1.4 MB/s eta 0:00:03\n",
            "   --------------------------- ------------ 8.1/12.0 MB 1.4 MB/s eta 0:00:03\n",
            "   --------------------------- ------------ 8.2/12.0 MB 1.4 MB/s eta 0:00:03\n",
            "   --------------------------- ------------ 8.2/12.0 MB 1.4 MB/s eta 0:00:03\n",
            "   --------------------------- ------------ 8.3/12.0 MB 1.4 MB/s eta 0:00:03\n",
            "   --------------------------- ------------ 8.3/12.0 MB 1.4 MB/s eta 0:00:03\n",
            "   ---------------------------- ----------- 8.4/12.0 MB 1.4 MB/s eta 0:00:03\n",
            "   ---------------------------- ----------- 8.5/12.0 MB 1.4 MB/s eta 0:00:03\n",
            "   ---------------------------- ----------- 8.6/12.0 MB 1.4 MB/s eta 0:00:03\n",
            "   ---------------------------- ----------- 8.6/12.0 MB 1.4 MB/s eta 0:00:03\n",
            "   ---------------------------- ----------- 8.7/12.0 MB 1.4 MB/s eta 0:00:03\n",
            "   ----------------------------- ---------- 8.8/12.0 MB 1.4 MB/s eta 0:00:03\n",
            "   ----------------------------- ---------- 8.9/12.0 MB 1.4 MB/s eta 0:00:03\n",
            "   ----------------------------- ---------- 8.9/12.0 MB 1.4 MB/s eta 0:00:03\n",
            "   ------------------------------ --------- 9.0/12.0 MB 1.4 MB/s eta 0:00:03\n",
            "   ------------------------------ --------- 9.1/12.0 MB 1.4 MB/s eta 0:00:03\n",
            "   ------------------------------ --------- 9.2/12.0 MB 1.4 MB/s eta 0:00:02\n",
            "   ------------------------------ --------- 9.3/12.0 MB 1.4 MB/s eta 0:00:02\n",
            "   ------------------------------- -------- 9.4/12.0 MB 1.4 MB/s eta 0:00:02\n",
            "   ------------------------------- -------- 9.4/12.0 MB 1.4 MB/s eta 0:00:02\n",
            "   ------------------------------- -------- 9.5/12.0 MB 1.4 MB/s eta 0:00:02\n",
            "   ------------------------------- -------- 9.6/12.0 MB 1.4 MB/s eta 0:00:02\n",
            "   ------------------------------- -------- 9.6/12.0 MB 1.4 MB/s eta 0:00:02\n",
            "   -------------------------------- ------- 9.7/12.0 MB 1.4 MB/s eta 0:00:02\n",
            "   -------------------------------- ------- 9.7/12.0 MB 1.4 MB/s eta 0:00:02\n",
            "   -------------------------------- ------- 9.8/12.0 MB 1.4 MB/s eta 0:00:02\n",
            "   -------------------------------- ------- 9.8/12.0 MB 1.4 MB/s eta 0:00:02\n",
            "   -------------------------------- ------- 9.9/12.0 MB 1.4 MB/s eta 0:00:02\n",
            "   -------------------------------- ------- 9.9/12.0 MB 1.4 MB/s eta 0:00:02\n",
            "   --------------------------------- ------ 9.9/12.0 MB 1.4 MB/s eta 0:00:02\n",
            "   --------------------------------- ------ 10.0/12.0 MB 1.4 MB/s eta 0:00:02\n",
            "   --------------------------------- ------ 10.1/12.0 MB 1.4 MB/s eta 0:00:02\n",
            "   --------------------------------- ------ 10.1/12.0 MB 1.4 MB/s eta 0:00:02\n",
            "   --------------------------------- ------ 10.2/12.0 MB 1.4 MB/s eta 0:00:02\n",
            "   ---------------------------------- ----- 10.2/12.0 MB 1.4 MB/s eta 0:00:02\n",
            "   ---------------------------------- ----- 10.3/12.0 MB 1.4 MB/s eta 0:00:02\n",
            "   ---------------------------------- ----- 10.4/12.0 MB 1.4 MB/s eta 0:00:02\n",
            "   ---------------------------------- ----- 10.4/12.0 MB 1.4 MB/s eta 0:00:02\n",
            "   ---------------------------------- ----- 10.5/12.0 MB 1.4 MB/s eta 0:00:02\n",
            "   ---------------------------------- ----- 10.5/12.0 MB 1.4 MB/s eta 0:00:02\n",
            "   ----------------------------------- ---- 10.6/12.0 MB 1.4 MB/s eta 0:00:02\n",
            "   ----------------------------------- ---- 10.6/12.0 MB 1.4 MB/s eta 0:00:01\n",
            "   ----------------------------------- ---- 10.7/12.0 MB 1.4 MB/s eta 0:00:01\n",
            "   ----------------------------------- ---- 10.7/12.0 MB 1.4 MB/s eta 0:00:01\n",
            "   ----------------------------------- ---- 10.8/12.0 MB 1.4 MB/s eta 0:00:01\n",
            "   ----------------------------------- ---- 10.8/12.0 MB 1.4 MB/s eta 0:00:01\n",
            "   ------------------------------------ --- 10.9/12.0 MB 1.4 MB/s eta 0:00:01\n",
            "   ------------------------------------ --- 11.0/12.0 MB 1.4 MB/s eta 0:00:01\n",
            "   ------------------------------------ --- 11.0/12.0 MB 1.4 MB/s eta 0:00:01\n",
            "   ------------------------------------- -- 11.1/12.0 MB 1.4 MB/s eta 0:00:01\n",
            "   ------------------------------------- -- 11.2/12.0 MB 1.4 MB/s eta 0:00:01\n",
            "   ------------------------------------- -- 11.2/12.0 MB 1.4 MB/s eta 0:00:01\n",
            "   ------------------------------------- -- 11.3/12.0 MB 1.4 MB/s eta 0:00:01\n",
            "   ------------------------------------- -- 11.4/12.0 MB 1.4 MB/s eta 0:00:01\n",
            "   -------------------------------------- - 11.4/12.0 MB 1.4 MB/s eta 0:00:01\n",
            "   -------------------------------------- - 11.5/12.0 MB 1.4 MB/s eta 0:00:01\n",
            "   -------------------------------------- - 11.6/12.0 MB 1.4 MB/s eta 0:00:01\n",
            "   -------------------------------------- - 11.7/12.0 MB 1.4 MB/s eta 0:00:01\n",
            "   -------------------------------------- - 11.7/12.0 MB 1.4 MB/s eta 0:00:01\n",
            "   ---------------------------------------  11.8/12.0 MB 1.4 MB/s eta 0:00:01\n",
            "   ---------------------------------------  11.9/12.0 MB 1.4 MB/s eta 0:00:01\n",
            "   ---------------------------------------  11.9/12.0 MB 1.4 MB/s eta 0:00:01\n",
            "   ---------------------------------------  12.0/12.0 MB 1.4 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 12.0/12.0 MB 1.4 MB/s eta 0:00:00\n",
            "Installing collected packages: tabula-py\n",
            "Successfully installed tabula-py-2.9.0\n"
          ]
        }
      ],
      "source": [
        "!pip install tabula-py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "11SeGlVKW5u6",
        "outputId": "5219e86b-dcb5-47ec-cbed-82c4f6302ea6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting python-docx\n",
            "  Using cached python_docx-1.1.0-py3-none-any.whl.metadata (2.0 kB)\n",
            "Collecting lxml>=3.1.0 (from python-docx)\n",
            "  Downloading lxml-5.1.0-cp311-cp311-win_amd64.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: typing-extensions in c:\\users\\devba\\miniconda3\\lib\\site-packages (from python-docx) (4.9.0)\n",
            "Using cached python_docx-1.1.0-py3-none-any.whl (239 kB)\n",
            "Downloading lxml-5.1.0-cp311-cp311-win_amd64.whl (3.9 MB)\n",
            "   ---------------------------------------- 0.0/3.9 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/3.9 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/3.9 MB 393.8 kB/s eta 0:00:10\n",
            "   - -------------------------------------- 0.1/3.9 MB 901.1 kB/s eta 0:00:05\n",
            "   - -------------------------------------- 0.2/3.9 MB 1.2 MB/s eta 0:00:04\n",
            "   --- ------------------------------------ 0.3/3.9 MB 1.4 MB/s eta 0:00:03\n",
            "   --- ------------------------------------ 0.4/3.9 MB 1.4 MB/s eta 0:00:03\n",
            "   ---- ----------------------------------- 0.5/3.9 MB 1.4 MB/s eta 0:00:03\n",
            "   ----- ---------------------------------- 0.5/3.9 MB 1.6 MB/s eta 0:00:03\n",
            "   ------ --------------------------------- 0.6/3.9 MB 1.4 MB/s eta 0:00:03\n",
            "   ------ --------------------------------- 0.6/3.9 MB 1.4 MB/s eta 0:00:03\n",
            "   ------ --------------------------------- 0.6/3.9 MB 1.4 MB/s eta 0:00:03\n",
            "   ------ --------------------------------- 0.7/3.9 MB 1.2 MB/s eta 0:00:03\n",
            "   ------- -------------------------------- 0.7/3.9 MB 1.3 MB/s eta 0:00:03\n",
            "   -------- ------------------------------- 0.8/3.9 MB 1.3 MB/s eta 0:00:03\n",
            "   --------- ------------------------------ 0.9/3.9 MB 1.3 MB/s eta 0:00:03\n",
            "   --------- ------------------------------ 1.0/3.9 MB 1.3 MB/s eta 0:00:03\n",
            "   ---------- ----------------------------- 1.0/3.9 MB 1.3 MB/s eta 0:00:03\n",
            "   ----------- ---------------------------- 1.1/3.9 MB 1.3 MB/s eta 0:00:03\n",
            "   ------------ --------------------------- 1.2/3.9 MB 1.4 MB/s eta 0:00:02\n",
            "   ------------ --------------------------- 1.3/3.9 MB 1.4 MB/s eta 0:00:02\n",
            "   ------------- -------------------------- 1.3/3.9 MB 1.4 MB/s eta 0:00:02\n",
            "   -------------- ------------------------- 1.4/3.9 MB 1.4 MB/s eta 0:00:02\n",
            "   -------------- ------------------------- 1.4/3.9 MB 1.4 MB/s eta 0:00:02\n",
            "   --------------- ------------------------ 1.5/3.9 MB 1.4 MB/s eta 0:00:02\n",
            "   ---------------- ----------------------- 1.6/3.9 MB 1.4 MB/s eta 0:00:02\n",
            "   ---------------- ----------------------- 1.7/3.9 MB 1.4 MB/s eta 0:00:02\n",
            "   ----------------- ---------------------- 1.7/3.9 MB 1.4 MB/s eta 0:00:02\n",
            "   ------------------ --------------------- 1.8/3.9 MB 1.4 MB/s eta 0:00:02\n",
            "   ------------------- -------------------- 1.9/3.9 MB 1.4 MB/s eta 0:00:02\n",
            "   ------------------- -------------------- 1.9/3.9 MB 1.4 MB/s eta 0:00:02\n",
            "   ------------------- -------------------- 2.0/3.9 MB 1.4 MB/s eta 0:00:02\n",
            "   -------------------- ------------------- 2.0/3.9 MB 1.4 MB/s eta 0:00:02\n",
            "   --------------------- ------------------ 2.1/3.9 MB 1.4 MB/s eta 0:00:02\n",
            "   ---------------------- ----------------- 2.2/3.9 MB 1.4 MB/s eta 0:00:02\n",
            "   ---------------------- ----------------- 2.2/3.9 MB 1.4 MB/s eta 0:00:02\n",
            "   ----------------------- ---------------- 2.3/3.9 MB 1.4 MB/s eta 0:00:02\n",
            "   ------------------------ --------------- 2.4/3.9 MB 1.4 MB/s eta 0:00:02\n",
            "   ------------------------- -------------- 2.5/3.9 MB 1.4 MB/s eta 0:00:02\n",
            "   ------------------------- -------------- 2.5/3.9 MB 1.4 MB/s eta 0:00:01\n",
            "   -------------------------- ------------- 2.6/3.9 MB 1.4 MB/s eta 0:00:01\n",
            "   -------------------------- ------------- 2.6/3.9 MB 1.4 MB/s eta 0:00:01\n",
            "   --------------------------- ------------ 2.7/3.9 MB 1.4 MB/s eta 0:00:01\n",
            "   ---------------------------- ----------- 2.8/3.9 MB 1.4 MB/s eta 0:00:01\n",
            "   ---------------------------- ----------- 2.8/3.9 MB 1.4 MB/s eta 0:00:01\n",
            "   ----------------------------- ---------- 2.9/3.9 MB 1.4 MB/s eta 0:00:01\n",
            "   ------------------------------ --------- 2.9/3.9 MB 1.4 MB/s eta 0:00:01\n",
            "   ------------------------------ --------- 3.0/3.9 MB 1.4 MB/s eta 0:00:01\n",
            "   ------------------------------- -------- 3.1/3.9 MB 1.4 MB/s eta 0:00:01\n",
            "   -------------------------------- ------- 3.2/3.9 MB 1.4 MB/s eta 0:00:01\n",
            "   --------------------------------- ------ 3.2/3.9 MB 1.4 MB/s eta 0:00:01\n",
            "   ---------------------------------- ----- 3.3/3.9 MB 1.4 MB/s eta 0:00:01\n",
            "   ---------------------------------- ----- 3.4/3.9 MB 1.4 MB/s eta 0:00:01\n",
            "   ----------------------------------- ---- 3.5/3.9 MB 1.4 MB/s eta 0:00:01\n",
            "   ----------------------------------- ---- 3.5/3.9 MB 1.4 MB/s eta 0:00:01\n",
            "   ------------------------------------ --- 3.6/3.9 MB 1.4 MB/s eta 0:00:01\n",
            "   ------------------------------------- -- 3.7/3.9 MB 1.4 MB/s eta 0:00:01\n",
            "   -------------------------------------- - 3.8/3.9 MB 1.4 MB/s eta 0:00:01\n",
            "   ---------------------------------------  3.9/3.9 MB 1.4 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 3.9/3.9 MB 1.4 MB/s eta 0:00:00\n",
            "Installing collected packages: lxml, python-docx\n",
            "Successfully installed lxml-5.1.0 python-docx-1.1.0\n"
          ]
        }
      ],
      "source": [
        "!pip install python-docx\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "o8SgPjI9TXxZ"
      },
      "outputs": [],
      "source": [
        "import io\n",
        "import pytesseract\n",
        "from pdf2image import convert_from_path\n",
        "from docx import Document\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rXSkj0tkTvXE",
        "outputId": "ea357fab-ebdb-46e2-878d-1af84a7bd6b8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ADITYA SHAH\n",
            "\n",
            "CONTACT\n",
            "\n",
            "| +919313449922\n",
            "\n",
            "Sx] 21BCEQO8@NIRMAUNI.AC.IN\n",
            "AHAS38@GMAIL.COM\n",
            "\n",
            "(es) www.reallygreatsite.com\n",
            "\n",
            "SOFT SKILLS\n",
            "e Highly Organized\n",
            "\n",
            "e Adabtablility\n",
            "\n",
            "e Honest\n",
            "\n",
            "e Collaborative\n",
            "\n",
            "e Accountable\n",
            "\n",
            "EDUCATION\n",
            "2021-present\n",
            "\n",
            "Nirma University\n",
            "B. Tech. Computer Science\n",
            "CGPA - 8.52 (till sem-3)\n",
            "\n",
            "2019-2021\n",
            "\n",
            "Shiv Ashish School\n",
            "Higher Secondary School\n",
            "\n",
            "12th Percentage - 89.4%\n",
            "\n",
            "2018-2019\n",
            "Udgam School For Children\n",
            "\n",
            "Senior Secondary School\n",
            "10th Percentage - 93%\n",
            "\n",
            "TECHNICAL SKILLS\n",
            "\n",
            "e Python and Java (Project\n",
            "Development)\n",
            "\n",
            "e HTML,CSS, Bootstrap and\n",
            "JavaScript\n",
            "e Cand C++(Problem Solving)\n",
            "\n",
            "e Machine Learning Algorithms,\n",
            "OpenCV\n",
            "\n",
            "COMPUTER SCIENCE\n",
            "\n",
            "STUDENT\n",
            "\n",
            "PROFILE\n",
            "\n",
            "| am a passionate Engineering student with an interest in coding as\n",
            "well as the business side of software development and project\n",
            "management with a keen interest in the field of finance. With my\n",
            "logical skills and a strong commitment to constant learning of new\n",
            "things | am seeking to create new solutions to different existing\n",
            "problems.\n",
            "\n",
            "WORK EXPERIENCE\n",
            "Summer Intern (2 weeks)\n",
            "\n",
            "4c Consulting Software Private Limited\n",
            "Worked in the field of Project Management and Business Analyst\n",
            "\n",
            "e Development of Business Requirement Document and Functional Requirement\n",
            "\n",
            "Document\n",
            "e Project Management Process\n",
            "\n",
            "e Story, Flowchart, Mock up and Wireframe development\n",
            "e Quality Analysis Testing\n",
            "\n",
            "e Software Module User Manual Development\n",
            "\n",
            "e Conversation with Customer\n",
            "\n",
            "PROJECTS\n",
            "\n",
            "e Train Management System - Java\n",
            "\n",
            "Using the concepts of error handling, inheritance , abstraction and class, train\n",
            "management system allows the user to book, delete and change the details of the\n",
            "ticket used for booking a ticket allowing the user to go through a list of places and\n",
            "be provided with the details of the same in a text file.\n",
            "\n",
            "e Movie Booking System - C\n",
            "\n",
            "The Booking System provided the user with the option to purchase a ticket and edit\n",
            "the details of the same with the help of viewing the entire theatre and showing the\n",
            "available as well as booked seats with the help of a static array.\n",
            "\n",
            "e Cab Management System - DBMS\n",
            "\n",
            "By the creation of different types of database tables required for consisting the\n",
            "details of various users namely the driver, the booking person and the customer\n",
            "care a cab management system was developed.\n",
            "\n",
            "e Flappy Bird using pygame - Python\n",
            "\n",
            "The project was developed with the help of pygame module and concepts of class\n",
            "along with the constant usage of different coordinates and creation of different\n",
            "objects and instances of the various items like the bird , pipes , ground etc.\n",
            "\n",
            "e Bank Management System - C++\n",
            "\n",
            "The management was a console based application, where a new user can create\n",
            "account and the users were stored in form of linked lists. They can make\n",
            "transactions like deposit, withdraw transfer amount to another person and apply for\n",
            "loan. The loan applications were prioritized according to heap tree algorithm based\n",
            "on the ratio of the requested amount and current balance of the user. The admin\n",
            "could then one by one give approval to the loan applicant ( at the root node )\n",
            "\f\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def extract_text_from_pdf(pdf_path):\n",
        "\n",
        "    pages = convert_from_path(pdf_path, 500)\n",
        "\n",
        "\n",
        "    text_data = ''\n",
        "    for page in pages:\n",
        "        text = pytesseract.image_to_string(page)\n",
        "        text_data += text + '\\n'\n",
        "\n",
        "\n",
        "    return text_data\n",
        "\n",
        "text = extract_text_from_pdf('21bce008.pdf')\n",
        "print(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zqeQzZGSWshN",
        "outputId": "df79543e-c7f7-4b77-e45b-bd05a82d5955"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                                                                      Sindhura Reddy\n",
            "Email: sindhuradataengineer@gmail.com                                                                                          PH:813-438-2039.\n",
            "Tampa, FL 33637 \n",
            "Visa Status - Green Card\n",
            "Sr. Data Engineer\n",
            "Professional Summary\n",
            "Having 9+ years of professional IT experience which includes in Big Data ecosystem and Python technologies.\n",
            "Expertise in Hadoop architecture and various components such as HDFS, YARN,  Hive, Pig, High Availability, Job Tracker, Task Tracker, Name Node, Data Node,  Apache, Cassandra, and MapReduce programming paradigm.\n",
            "Experience in Hadoop cluster using Cloudera’s CDH, Hortonworks HDP.\n",
            "Worked on Airflow 1.8(Python2) and Airflow 1.9(Python3) for orchestration and am familiar with building custom Airflow operators and orchestrating workflows with dependencies involving multi-clouds.\n",
            "Experience in using various tools like Sqoop, Flume, Kafka, and Pig to ingest structured, semi-structured, and unstructured data into the cluster.\n",
            "Having hands-on experience in versioning using bit bucket.\n",
            " Good understanding of Cloud Based technologies such as AWS, GCP, and Azure.\n",
            "Proficient with Apache Spark ecosystems such as Spark and Spark Streaming using  Scala and Python.  · Developed highly optimized Spark applications to perform various data cleansing, validation, transformation, and summarization activities according to the requirement.\n",
            "Hands-on Experience in Spark architecture and its integrations like Spark SQL,  Data Frames, and Datasets APIs.\n",
            "Extensive experience in migrating on-premise Hadoop platforms to cloud solutions using AWS and GCP. \n",
            "Profound knowledge of Root Cause Analysis, Metadata Analysis, Tableau Dashboard, Tableau Calculated Fields, SQL, and Tableau Blend for code automation, customized reports, code re-usability, and Ad-hoc reports. \n",
            "Experience in RDD architecture and implementing spark operations on RDD also optimizing transformations and actions in Spark. \n",
            "Hands-on experience working on statistical procedures in R like ggplot, shiny apps, and dashboard building.\n",
            "Expertise in creating and customizing Splunk applications, searches, and dashboards as desired by IT teams and businesses. \n",
            "Experience collecting log data from various sources and integrating it into HDFS using Flume; staging data in HDFS for further analysis. \n",
            "Designed and developed logical and physical data models that utilize concepts such as Star Schema, Snowflake Schema and Slowly Changing Dimensions. \n",
            "Experience in Infrastructure Development and Operations involving AWS Cloud Services, EC2, EBS, VPC, RDS, SES, ELB, Auto scaling, CloudFront, Cloud Formation, Elastic Cache, API Gateway, Route 53, Cloud Watch, SNS.\n",
            "Experience in changing over existing AWS infrastructure to Serverless architecture (AWS Lambda, AWS Kinesis) through the creation of a Serverless Architecture using AWS Lambda, API gateway, Route 53, S3 buckets.\n",
            "Proficiency in SQL across several dialects (we commonly write MySQL, PostgreSQL, Redshift, SQL Server, and Oracle)\n",
            "5+ years of experience in writing python as an ETL framework and Pyspark to process huge amounts of data daily.\n",
            "Strong experience in implementing data models and loading unstructured data using HBase, Dynamo DB, and Cassandra.\n",
            "Having hands-on experience in Application Deployment using the CICD pipeline.\n",
            "Can work parallel in both GCP and Aws Clouds coherently.  · Experience in implementing Spark using Scala and Spark SQL for faster processing of data.\n",
            "Strong experience in extracting and loading data using complex business logic using Hive from different data sources and building ETL pipelines to process terabytes of data daily.\n",
            "Experienced in transporting and processing real-time event streaming using  Kafka and Spark Streaming.  · Hands-on experience with importing and exporting data from Relational databases to HDFS, Hive, and HBase using Sqoop.\n",
            "Experienced in processing real-time data using Kafka 0.10.1 producers and stream processors and implemented stream process using Kinesis and data landed into Data  Lake S3.  \n",
            "Implemented various algorithms for analytics using Cassandra with Spark and Scala.\n",
            "Strong Experience in implementing Data warehouse solutions in Confidential Redshift; Worked on various projects to migrate data from on-premise databases to Confidential Redshift, RDS, and S3.\\\n",
            "Adept in statistical programming languages like R and Python including Big Data Technologies like Hadoop 2, HAVE, HDFS, MapReduce, and Spark.\n",
            "Expertise in building CI/CD on AWS environment using AWS Code Commit, Code Build, Code Deploy, and Code Pipeline and experience in using AWS CloudFormation, API Gateway, and AWS Lambda in automation and securing the infrastructure on AWS.\n",
            "Created Automation to create infrastructure for Kafka clusters with different instances as per components in the cluster using Terraform for creating multiple EC2 instances & attaching ephemeral or EBS volumes as per instance type in different availability zones & multiple regions in AWS\n",
            "Experienced in configuring and administering the Hadoop Cluster using major Hadoop Distributions like Apache Hadoop and Cloudera.\n",
            "\n",
            "        Professional Experience\n",
            "        Data Engineer\n",
            "        KPMG, Tampa FL                                                                                                                                        May 2020 to Present\n",
            "        Responsibilities:\n",
            "Experienced in writing Spark Applications in Scala and Python.\n",
            "Build data pipelines in airflow in GCP for ETL-related jobs using different airflow operators.\n",
            "Experience in GCP Dataproc, GCS, Cloud functions, BigQuery · Analyzed large and critical datasets using HDFS MapReduce, Kafka, Spark, HBase, Hive, Hive UDF, and Spark. · Used Kafka consumer’s API in Scala for consuming data from Kafka topics.\n",
            "Developed spark applications in Spark on the distributed environment to load a huge number of CSV files with different schema into Hive ORC tables.\n",
            "Designed SSIS Packages to transfer data from flat files, Excel SQL Server using Business Intelligence Development Studio. · Developed a POC for project migration from the on-premise Hadoop MapR system to GCP.\n",
            "Analyzed the SQL scripts and designed the solution to implement using Pyspark. · Used cloud shell SDK in GCP to configure the services Data Proc, Storage, and BigQuery.\n",
            "Coordinated with the team and Developed a framework to generate Daily Adhoc reports and Extracts from enterprise data from BigQuery.\n",
            "  Developed custom aggregate functions using Spark SQL and performed interactive querying.\n",
            "Implemented Apache Airflow for authoring, scheduling, and monitoring Data Pipelines · Developed Python code to gather the data from HBase (Cornerstone) and designed the solution to implement using PySpark.\n",
            "Ran data formatting scripts in Java and created terabyte CSV files to be consumed by Hadoop MapReduce jobs. · Implemented Kafka model which pulls the latest records into Hive external tables.\n",
            "Loaded all datasets into Hive from Source CSV files using spark and Cassandra from Source CSV files using Spark/PySpark. · Develop Cloud Functions in Python to process JSON files from source and load the files to BigQuery.\n",
            "Involved in ETL, Data Integration, and Migration by writing Pig scripts.\n",
            "Exported the analyzed data to Teradata using Sqoop for visualization and to generate reports for the BI team. · Migrated the computational code in HQL to PySpark.\n",
            "Imported data into HDFS from various SQL databases and files using Sqoop and from streaming systems using Storm into Big Data Lake.\n",
            "Work related to downloading BigQuery data into pandas or Spark data frames for advanced ETL capabilities · Completed data extraction, aggregation, and analysis in HDFS by using PySpark and storing the data needed to Hive.\n",
            "Exposure to the usage of Apache Kafka develops a data pipeline of logs as a stream of messages using producers and consumers. · Sound knowledge in programming Spark using Scala.\n",
            "Populated HDFS and HBase with huge amounts of data using Apache Kafka.\n",
            "Experienced in working with various kinds of data sources such as Teradata and Oracle. Successfully loaded files to HDFS from Teradata and load loaded from HDFS to Hive and Impala. \n",
            "Installed Oozie workflow engine to run multiple Hive and Pig jobs which run independently with time and data availability.\n",
            "Involved in implementing security on Hortonworks Hadoop Cluster using Kerberos by working along with operations team to move nonsecured cluster to secured cluster.\n",
            "Created Build and Release for multiple projects in a production environment using Visual Studio Team Services\n",
            "Implemented schema extraction for Parquet and Avro file Formats in Hive/MongoDB.\n",
            "Utilized Kubernetes and Docker for the runtime environment for the CI/CD system to build, test, and deploy.\n",
            "Written Spark applications using Scala to interact with the PostgreSQL database using Spark SQL Context and accessed Hive tables using Hive Context.\n",
            "Used Spark Streaming to receive real-time data from Kafka and store the stream data to HDFS using Python and NoSQL databases such as HBase and Cassandra.\n",
            "Experience in integrating real-time data using Kafka and Spark Streaming in Java and Scala to process it coming from different source systems such as XLSX, CSV, Database, etc.\n",
            "Developed PIG and Hive UDFs in java for extended use of PIG and Hive and wrote Pig Scripts for sorting, joining, filtering, and grouping the data.\n",
            "Worked and troubleshot in establishing the connections between Cognos BI and AWS Athena.\n",
            "Automated builds using Maven and scheduled automated nightly builds using Jenkins. Built Jenkins pipeline to drive all microservices builds to the Docker registry and then deployed to Kubernetes.\n",
            "Involved in POC Data Extraction, aggregations, and consolidation of data within AWS Glue using PySpark.\n",
            "As a Tableau developer, supported customer service designing ETL jobs dashboards utilizing data from Redshift.\n",
            "Responsible for real-time heterogeneous replication from MySQL to Oracle DB and MySQL to AWS Redshift using the Tungsten replicator tool while performance tuning and maintaining both databases.\n",
            " Created Redshift clusters on AWS for quick accessibility for reporting needs. Designed and deployed a Spark cluster and different Big Data analytic tools including Spark, Kafka streaming, AWS, and HBase with Cloudera Distribution.\n",
            "Environment: Spark RDD. Python, Hadoop, AWS Glue, Apache Kafka, Amazon S3, SQL, Spark, AWS cloud, GCP, Bigquery, Kafka, SQL, Cloudera, Cassandra, Tableau, NoSQL, redshift, ETL, and Spark 1.6 / 2.0 (PySpark, MLlib, EMR, EC2, and Amazon RDS). Data lake, Cloudera Stack, HBase, Hive, Impala, Pig, NiFi, Spark, Spark Streaming, ElasticSearch, Teradata, Logstash, Kibana, JAX-RS, Spring, Ubuntu, and Solar.\n",
            "\n",
            "       Sr. Data Engineer\n",
            "       Walmart, Sunnyvale CA                                                                                                                           June 2018 to April 2020\n",
            "       Responsibilities:\n",
            "Developed real-time data processing applications by using Scala and Python and implemented Apache Spark Streaming from various streaming sources like Kafka and JMS.\n",
            "Knowledge of PySpark and used Hive to analyze sensor data and cluster users based on their behavior in the events.\n",
            "Created BigQuery authorized views for row-level security or exposing the data to other teams.\n",
            "Experienced in writing live Real-time Processing and core jobs using Spark Streaming with Kafka as a data pipeline system.\n",
            "Involved in converting Hive/SQL queries into Spark transformations using Spark RDDs and Scala.\n",
            "Experience in building and architecting multiple Data pipelines, end-to-end ETL process for Data ingestion and transformation in GCP, and coordinating tasks among the team.\n",
            "Experience in building and architecting multiple Data pipelines, end-to-end ETL, and ELT processes for Data ingestion and transformation in GCP.\n",
            "Build a program with Python and apache beam and execute it in cloud Dataflow to run Data validation between the raw source files and BigQuery tables.\n",
            "Involved in loading data from Linux file systems, servers, and web services using Kafka producers and partitions.\n",
            "Applied Kafka custom encoders for custom input format to load data into Kafka Partitions.\n",
            "Implement POC with Hadoop. Extract data with Spark into HDFS.\n",
            "Used Spark SQL with Scala for creating data frames and performed transformations on data frames.\n",
            "Used Spark API over Cloudera Hadoop YARN to perform analytics on data in Hive.\n",
            "Developed code to read data stream from Kafka and send it to respective bolts through the respective stream.\n",
            "Creating Spark clusters and configuring high concurrency clusters using Azure Databricks to speed up the preparation of high-quality data.\n",
            "Developed Spark applications using Scala for easy Hadoop transitions.\n",
            "Optimized the code using PySpark for better performance.\n",
            "Used cloud shell SDK in GCP to configure the services Data Proc, Storage, BigQuery · Worked on Spark streaming using Apache Kafka for real-time data processing.\n",
            "Experienced in optimizing Hive queries and joins to handle different data sets.\n",
            "Involved in ETL, Data Integration, and Migration by writing Pig scripts.\n",
            "Developed MapReduce programs to cleanse the data in HDFS obtained from heterogeneous data sources.\n",
            "Designed and implemented MongoDB and associated RESTful web service.\n",
            "Involved in writing test cases and implementing test classes using MR Unit and mocking frameworks.\n",
            "Developed Shell, Perl, and Python scripts to automate and provide Control flow to Pig scripts.\n",
            "Used Talend tool to create workflows for processing data from multiple source systems.\n",
            "\n",
            "Environment: Apache Hadoop 0.20.203, GCP, Python, AWS, Hbase, Cassandra, MySQL, SQL, Kafka, Spark, Redshift, \n",
            "Snowflake, Data Modeling, BigQuery, Kafka, Scala, Cloudera Manager (CDH4), HDFS, Eclipse, Hive, Pig, Sqoop, Oozie and SQL, Oracle 11g, Apache Spark, JMS, Azure SQL Data Warehouse, HDFS, Spark API, Shell.\n",
            "\n",
            "       Python Developer/Data Engineer\n",
            "       Charter, MO                                                                                                                                                  Feb 2017 to May 2018\n",
            "       Responsibilities:\n",
            "Integrated data from various system source flat files, CSV, and Oracle using PySpark. Created efficient documentation for process/procedure rules.\n",
            "Implemented data pipeline for processing data objects into S3 data bucket with security policies and ensuring uniformity of data.Developed test scripts using python and postgreSQL for the data validation. \n",
            "Written the python scripts to load the data in to the tables. \n",
            "Used Autosys and MoveIT tools to place the file from on premise to AWS-S3 buckets. \n",
            "Written the jil scripts to autosys jobs and scheduled jobs on every business day. \n",
            "Used celery scheduler to places the file into tables from AWS-S3. \n",
            "Used AWS Athena to query the data on top s3 bucket (files in inbound) which helps the business people for data analysis. \n",
            "Written postgreSQL sequels according to the business transformations to place the data in to respective tables. \n",
            "Created the splunk dashboard for run time errors directed from AWS cloud using kinesis. \n",
            "Involved in software development life cycle (SDLC) of tracking the requirements, gathering, analysis, detailed design, development, system testing and user acceptance testing. \n",
            "Actively involved in developing the methods for Create, Read, Update and Delete \n",
            "(CRUD) in Active Records. \n",
            "Developed entire frontend and backend modules using Python on Django Web \n",
            "Framework by implementing MVC architecture. \n",
            "Rewrote existing python/Flask module to deliver certain format of data. \n",
            "Created script in python for calling REST APIs. \n",
            "Deployed Django web application in Apache webserver and carpathia cloud web deployment. \n",
            "Installed Kafka manager for consumer lags and for monitoring kafka metrics also this has been used for adding topics, Partitions etc. \n",
            "· Installed Kafka manager for consumer lags and for monitoring Kafka metrics also this has been used for adding topics, Partitions etc. \n",
            "Involved in designing user interactive web pages as the frontend part of the web application using various web technologies like HTML, JavaScript, JQuery and implement CSS for better appearance and feel. \n",
            "Used Celery as task queue and RabbitMQ, Redis as messaging broker to execute asynchronous tasks. \n",
            "Worked on MongoDB database concepts such as locking, transactions, indexes, \n",
            "Sharding, replication, schema design. \n",
            "Managed datasets using Panda data frames and MySQL, queried MYSQL database queries from python using python-MySQL connector and MySQLdb package to retrieve information. \n",
            "Worked with external vendors or partners to onboard external data into Target S3 buckets. \n",
            "Worked on Oozie to develop workflows to automate the ETL data pipeline. \n",
            "Perform Data Cleaning, features scaling and features engineering using pandas and NumPy packages in python.\n",
            "Successfully set up a no authentication Kafka listener in parallel with Kerberos (SASL) Listener. Also, I tested a non-authenticated user (an Anonymous user) in parallel with the Kerberos user.\n",
            "Advanced knowledge of Confidential Redshift and MPP database concepts.\n",
            "Migrated on-premise database structure to Confidential Redshift data warehouse\n",
            "Implemented Workload Management (WML) in Redshift to prioritize basic dashboard queries over more complex longer-running ad hoc queries. This allowed for a more reliable and faster reporting interface, giving sub-second query responses for basic queries.\n",
            "Day-to-day responsibility includes developing ETL Pipelines in and out of the data warehouse and developing major regulatory and financial reports using advanced SQL queries in snowflake.\n",
            "Stage the API or Kafka Data(in JSON file format) into Snowflake DB by Flattening the same for different functional services.\n",
            "Worked on continuous Integration tools Jenkins and automated jar files at the end of the day.\n",
            "Heavily involved in testing Snowflake to understand the best possible way to use the cloud resources.\n",
            "Worked extensively on AWS Components such as Airflow, Elastic Map Reduce (EMR), Athena, and Snowflake.\n",
            "Implemented various Data Modeling techniques for Cassandra.\n",
            "Possess good knowledge in creating and launching EC2 instances using AMIs of Linux, Ubuntu, RHEL, and Windows and wrote shell scripts to the bootstrap instances.\n",
            "Design an ELK system to monitor and search enterprise alerts. Installed, configured, and managed the ELK Stack for Log management within EC2 / Elastic Load balancer for Elastic Search.\n",
            "Loaded data into redshift tables from s3, DynamoDB, EMR, and EC2 using copy utility.\n",
            "Worked on MongoDB database concepts such as locking, transactions, indexes, Sharding, replication, and schema design. Created multiple databases with shared collections and chose shard key based on the requirements. Experience in managing MongoDB environment from availability, performance, and scalability perspectives.\n",
            "Applied write concern for the level of acknowledgment while MongoDB writes operations to avoid rollback.\n",
            "Integrated with UI layer using HTML, Ajax, JavaScript\n",
            "Created multi-tier java-based multiple web services to read data from MongoDB.\n",
            "Lead role in NoSQL column family design, client access software, and Cassandra tuning; during migration from Oracle-based data stores.\n",
            "CAT EDH team's goal is to get rid of all their Teradata stacks and have all data on Amazon’s S3 storage platform.\n",
            "Used AWS Redshift, S3, Spectrum, and Athena services to query large amounts of data stored on S3 to create a Virtual Data Lake without having to go through the ETL process.\n",
            "Automate AWS infrastructure through infrastructure as code by writing various Terraform modules and scripts by creating AWS IAM users, groups, roles, policies, custom policies, AWS Glue, Crawlers, Redshift clusters, snapshots of clusters, EC2, and S3 buckets.\n",
            "Published Power BI Reports in the required originations and Made Power BI Dashboards available in Web clients and mobile apps\n",
            "Create ETL Mappings for the Operational dashboard for various KPIs and Business Metrics, allowing powerful drill down for Detail reports to understand the data at a very detailed level.\n",
            "Used Informatica Power Center Workflow manager to create sessions, workflows, and batches to run with the logic embedded in the mapping.\n",
            "Develop ETL pipelines in and out of the data warehouse using a combination of Python and Snowflake’s SnowSQL\n",
            "Extract Transform and Load data from sources Systems to Azure Data Storage services using a combination of Azure Data factory, T-SQL, Spark SQL, and U-SQL Azure Data Lake Analytics. Data ingestion to one or more Azure services (Azure Data Lake, Azure Storage, Azure SQL, Azure DW)and processing the data in Azure Databricks.\n",
            "Implemented Security roles on SSAS cubes and Power BI services by implementing authentication and authorization methods.\n",
            "As a BI Developer, I was involved in developing different reports using Microsoft BI tools like SSIS and SSRS and developed Power BI dashboards that would help businesses operate smoothly.\n",
            "Used APIs with Python to extract and push prediction outputs in EMR.\n",
            "Created continuous integration and continuous delivery (CI/CD) pipeline on AWS that helps to automate steps in the software delivery process.\n",
            "Upgraded from Cloudera manager and CDH from 5.7.1 to CDH 5.11.2 worked with Cloudera to set up SSO login and worked on data migration in HIVE. worked on Importing and exporting data into HDFS and Hive using Sqoop.\n",
            "Environment: CSV, Oracle, PySpark, Spark RDD, S3. JSON, Parquet, ETL, Hive, UNIX, HDFS, Power BI, SQL Server. DevOps, Microsoft Azure, Kerberos, Scala, Python, AWS, Kafka, Redshift, Airflow, Snowflake, Cassandra, Map Reduce (EMR), AWS Athena, Linux, Ubuntu, EC2, MongoDB, JavaScript, Ajax, CAT, AWS Glue, Crawlers, Power Center SnowSQL, Cloudera, Jenkins. data bricks.\n",
            "       Data Engineer\n",
            "       ViacomCBS, NYC                                                                                                                                        Jan 2017 to Dec 2017\n",
            "       Responsibilities: \n",
            "Involved in Requirement Gathering, Design, and Deployment of the application using Scrum (Agile) as a Development methodology. \n",
            "Wrote Spark codes in Java to run a sorting application on the data stored on AWS. \n",
            "Hands-on experience in writing Java-based Kafka and Spark streaming programs\n",
            "Used Spark Streaming to divide streaming data into batches as an input to the Spark engine for batch processing. \n",
            "Designed batch processing jobs using Apache Spark to increase speed compared to that MapReduce jobs. \n",
            "Wrote Spark applications for data validation, cleansing, transformation, and custom aggregation. \n",
            "Automated all the jobs for pulling data from the FTP server to load data into Hive tables, using Oozie workflows. \n",
            "Involved in scheduling Oozie workflow engine to run multiple HiveQL, Sqoop, and Pig jobs. \n",
            "Designed HBase row key and data modeling to insert to HBase tables using concepts of lookup tables and staging tables. \n",
            "Developed data pipeline using Sqoop to ingest customer behavioral data and purchase histories into HDFS for analysis. \n",
            "Collected the JSON data from HTTP Source and developed Spark APIs that help to do inserts and updates in Hive tables. \n",
            "Developed shell scripts for running Hive scripts in Hive and Impala. \n",
            "Designed new VPCs for the secure access POCs, and architected and implemented multiple Amazon Glue/Spark pipelines with 100s DPUs.\n",
            "Involved in importing the real-time data to Hadoop using Kafka and implemented the Oozie job for daily imports. \n",
            "Used Azure Data Factory as an orchestration tool for integrating data from upstream to downstream systems. \n",
            "Automated jobs using different triggers (Event, Scheduled, and Tumbling) in ADF. \n",
            "Used Cosmos DB for storing catalog data and event sourcing in order processing pipelines. \n",
            "Designed and developed user-defined functions, stored procedures, and triggers for Cosmos DB \n",
            "Created DA specs and Mapping Data flow and provided the details to the developer along with HLDs. \n",
            "Implemented AWS Elastic Container Service (ECS) scheduler to automate application deployment in the cloud using Docker Automation techniques.\n",
            "ETL Restarting capability for a date or date range or from the point of failure or from the beginning.\n",
            "Extract data from different social analytic sites (Facebook, Google+, Twitter, YouTube, Ooyala, iTunes, Google Analytics, Sony games) through Pentaho DI (ETL)\n",
            "Conducted statistical analysis on healthcare data using python and various tools.\n",
            "Responsible for building scalable distributed data solutions using Hadoop.\n",
            "Develop a big data web application using Agile methodology in Scala as Scala has the capability of combining functional and object-oriented programming.\n",
            "Responsible for Designing Logical and Physical data modeling for various data sources on Confidential \n",
            "Designed and Developed ETL jobs to extract data from Salesforce replica and load it in data mart in Redshift.\n",
            "Integrated Cassandra as a distributed persistent metadata store to provide metadata resolution for network entities on the network\n",
            "Queried and analyzed data from DataStax Cassandra for quick searching, sorting, and grouping.\n",
            "Data Migration from existing Teradata Systems to Hortonworks HDInsight cluster on Azure\n",
            "Monitor Resources and Applications using AWS Cloud Watch, including creating alarms to monitor metrics such as EBS, EC2, ELB, RDS, S3, and SNS, and configured notifications for the alarms generated based on events defined\n",
            "Used Confidential EMR to create spark clusters and EC2 instances and imported data stored In S3\n",
            "All the jobs are integrated using complex Mappings and Workflows using Informatica power center designer and workflow manager.\n",
            "Created and provisioned different Databricks clusters, notebooks, jobs, and autoscaling.\n",
            "Created several Databricks Spark jobs with PySpark to perform several tables-to-table operations.\n",
            "Implemented UDFS, UDAFS, and UDTFS in java for the hive to process the data that can’t be performed using Hive's inbuilt functions\n",
            "Manage MongoDB upgrades from 2.6 to 3.2, including hardware migrations.\n",
            "Setting up DevOps pipelines for CI/CD on GIT, Jenkins, Nexus repository\n",
            "I have integrated product data feeds from Kafka to the Spark processing system and stored the order details in the PostgreSQL database.\n",
            "Environment: Agile, AWS, Spark, Hive, Oozie, MapReduce, FTP, HiveQL, Sqoop, Pig, Amazon S3, HDFS, Cosmos, ETL scripts, MYSQL, Kafka, Cassandra, T-SQL, Spark SQL, and U-SQL. Azure Data Factory. Teradata, Cloud. EBS, EC2, ELB, RDS, S3, SNS, S3, Azure Data bricks, informatica,redshift,python,PySpark,scala,MongoDB,java, Jenkins,data bricks.\n",
            "       Big Data Engineer\n",
            "       WebSoc Technologies, Hyderabad India                                                                                             July 2014 to Dec 2016\n",
            "       Responsibilities: \n",
            "Worked on analyzing data in the Hadoop cluster using big data tools including Pig, Hive, and Sqoop. \n",
            "Collected large amounts of log data using Apache Flume/Sqoop and aggregated using Pig/Hive in HDFS for further analysis. \n",
            "Worked on analyzing and writing Hadoop MapReduce jobs using Java API, Pig, and Hive. \n",
            "Developed Spark SQL applications to perform complex data operations on structured and semi-structured data stored as Parquet. \n",
            "Extensively worked on creating combiners, partitioning, and Distributed cache to improve the performance of MapReduce jobs. \n",
            "Used Pig to do data transformations, event joins filter bot traffic, and some pre-aggregations before storing the data onto HDFS. \n",
            "Developed data aggregation metrics using Data frames and Dataset APIs of Spark. \n",
            "Tuned Spark application performance by setting the correct amount of memory and no. of executors. \n",
            "Coordinated with the project management team on database development timelines and project scope. \n",
            "Worked with Impala for querying the hive meta store data. \n",
            "Creating pipelines, data flows, and complex data transformations and manipulations using ADF and PySpark with Databricks \n",
            "Created and provisioned different Databricksclusters needed for batch and continuous streaming data processing and installed the required libraries for the clusters. \n",
            "Improved performance by optimizing computing time to process the streaming data and saved cost to the company by optimizing the cluster run time. \n",
            "Responsible for analysis of requirements and designing generic and standard ETL processes to load data from different source systems\n",
            "Involved in developing and documenting the ETL (Extract, Transformation, and Load) strategy to populate the Data Warehouse from various source systems\n",
            "Coordinating and leading ETL (Extract-Transform-Load) Development strategy for implementation of the Design requirement.\n",
            "Conducted statistical analysis to validate data and interpretations using Python and R, as well as presented Research findings, and status reports and assisted with collecting user feedback to improve the processes and tools.\n",
            "Implement code in Python to retrieve and manipulate data.\n",
            "Integrated Flume with Kafka and Worked on monitoring and troubleshooting the Kafka-Flume-HDFS data pipeline for real-time data ingestion in HDFS\n",
            "Worked on AWS Data Pipeline to configure data loads from S3 to Redshift\n",
            "Used JSON schema to define table and column mapping from S3 data to Redshift\n",
            "Optimizing and tuning the Redshift environment, enabling queries to perform up to 100x faster for Tableau and SAS Visual Analytics\n",
            "Provided technical solutions on MS Azure HDInsight, Hive, HBase, MongoDB, Telerik, Power BI, Spot Fire, Tableau, Azure SQL Data Warehouse Data Migration Techniques using BCP, Azure Data Factory, and Fraud prediction using Azure Machine Learning.\n",
            "User Management - creating users, assigning roles, managing permissions for SQL Server, MongoDB database server.\n",
            "Designed various Jenkins jobs to continuously integrate the processes and executed CI/CD pipelines using Jenkins.\n",
            "Set up Azure Repo and Pipelines for CI/CD deployment of objects.\n",
            "Transformed batch data from several tables containing tens of thousands of records from SQL Server, MySQL, PostgreSQL, and CSV file datasets into data frames using PySpark.\n",
            "Worked with NoSQL databases like HBase in creating tables to load large sets of semi-structured data coming from source systems.\n",
            "Integrated Salesforce.com with external systems like Oracle and SAP   using SOAP API and REST API.\n",
            "Worked with  Cloudera 5.12.x and its different components.\n",
            "Installation and setup of multi-node Cloudera cluster on AWS cloud.\n",
            "Hand full experience on AWS services like S3 bucket, lambda, AWS Glue, Cognito, Step Function, RedShift, and Cloud Watch using CDK\n",
            "Implemented Data Quality framework using AWS Athena, Snowflake, Airflow, and Python.\n",
            "Created ETL jobs on AWS glue to load vendor data from different sources, transformations involving data cleaning, data imputation, and data mapping and storing the results into S3 buckets. The stored data was later queried using AWS Athena.\n",
            "Environment: Hadoop Framework, CDH 5.4.0, 5.4.1, Hive, Pig, Impala, Flume, Oozie, Kafka, Spark, HBase, Java (JDK1.7, 1.8), UNIX Shell Scripting, Windows7, Linux RedHat, Eclipse, Oracle, SAP, SQL Server, Cloudera, AWS. ETL, YARN, Spark SQL, python, redshift, Azure, MongoDB, CI/CD, HDFS. RDD, Data Bricks, R programming, Athena, Power BI, Tableau, PySpark, snowflake, AWS glue, S3, Jenkins, data bricks.\n",
            "        Data Engineer\n",
            "        ARISSOFT SOLUTIONS, Hyderabad, India                                                                                        May 2013 to June 2014\n",
            "        Responsibilities:\n",
            "Analyzed requirements and prepared test cases with the purpose of finding defects in software for a banking client. The test cases developed for the application ensured a defect-free application for customers. \n",
            "Implemented Data Preparation team which improved test data for testing which improved testing by 6%. \n",
            "Delivered a quality product after a month of working with a client with minimal direction from the client and acceptance from the client. \n",
            "Created new datasets from raw datasets files using PROC IMPORT, LIBNAME, INFILE, and fetched data from data warehouses; manipulated customer datasets using SQL and DATA STEP statements such as SET and MERGE. Decreased execution time by creating and utilizing multiple macros. \n",
            "Worked on SAS reporting procedures like PROC MEANS, PROC SORT, PROC TRANSPOSE, PROC PRINT, and PROC TABULATE. \n",
            "Created numerous pipelines in Azure using Azure Data Factory v2 to get the data from disparate source systems by using different Azure Activities like Move &Transform, Copy, filter, for each, Databricks, etc. \n",
            "Created several Databricks Spark jobs with PySpark to perform several tables-to-table operations. \n",
            "Importing and exporting data jobs to perform operations like copying data from HDFS and to HDFS using Sqoop and developed Spark code and Spark-SQL/Streaming for faster testing and processing of data.\n",
            "Use Kafka a publish-subscribe messaging system by creating topics using consumers and producers to ingest data into the application for Spark to process the data and create Kafka topics for application and system logs.\n",
            "Advanced knowledge of Confidential Redshift and MPP database concepts.\n",
            "Analyze escalated incidences within the Azure SQL database. Implemented test scripts to support test-driven development and continuous integration.\n",
            "Responsible for ingesting data from various source systems (RDBMS, Flat files, BigData) into Azure (Blob Storage) using the framework model.\n",
            "Played a key role in migration from Adempiere LTS customizing and optimizing the new version to match the same functionalities as the legacy System, transferring the data from PostgreSQL to an Oracle Database, and building Restful web services as well.\n",
            "Deployments of Salesforce code in different sandboxes - DevOps Engineer.\n",
            "Developed and implement of Web-Lead, Lead conversion, and Entire Sales Process of the End-to-End project with Full Lifecycle Implementation of Salesforce.\n",
            "Designed EMR/Hadoop/Spark process to convert files to parquet and query on s3 using Athena, SQS, and Redshift Spectrum.\n",
            "To help developers automatically build and deploy software into production multiple times a day safely while maintaining compliance in a highly regulated financial industry. We use tools like Atlassian Bamboo, Bitbucket, Confluence, JIRA, Jenkins, Sonar type Nexus and Nexus IQ, SonarQube, Grunt, and Maven to get the job done.\n",
            "Used Amazon EMR for map reduction jobs and test locally using Jenkins.\n",
            "Used Jenkins and pipelines which helped us drive all Microservices builds out to the Docker registry and then deployed to Kubernetes.\n",
            "Setting up of CI/CD pipeline using continuous integration tools such as Cloud Bees Jenkins and automated the entire AWS EC2, VPC, S3, SNS, RedShift, EMR based infrastructure using Terraform, Chef, Python, Shell, Bash scripts and managing security groups on AWS and custom monitoring using Cloud Watch.\n",
            "Created data pipeline and ETL process to load Data from AWS S3 to RDS and redshift.\n",
            "Develop Informatica mappings to be implemented based on client requirements and for the analytics team.\n",
            "Develop Informatica SCD Type-2 Mappings based on the requirements.\n",
            "Involved in understanding requirements and in modeling activities of the attributes identified from different source systems which are in Oracle, Teradata, and CSV FILES. Data is Staged, integrated, Validated, and finally loaded the data into Teradata Warehouse using Informatica and Teradata Utilities.\n",
            "Environment: Windows 7,windows 8. PROC IMPORT, LIBNAME, INFILE, ca, PROC REPORT, PROC FREQ, Azure Data Factory, AWS Redshift, S3, Spectrum and Athena, Mspark. RDBMS, Flat files, Big Data, Spark jobs, Databricks, python, kafka, Pyspark, SQL, ETL, EC2, Jenkins.\n",
            "\n",
            "Education: Bachelors of Technology in Electronics and Communication Engineering from JNTU ANANTAPUR(2009-2013).\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def extract_text_from_docx(docx_path):\n",
        "    # Load DOCX file\n",
        "    doc = Document(docx_path)\n",
        "    text_data = ''\n",
        "\n",
        "    # Extract text from each paragraph\n",
        "    for para in doc.paragraphs:\n",
        "        text_data += para.text + '\\n'\n",
        "\n",
        "    return text_data\n",
        "\n",
        "text = extract_text_from_docx('/content/Sindhu SDE Resume.docx')\n",
        "print(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting bs4\n",
            "  Downloading bs4-0.0.2-py2.py3-none-any.whl.metadata (411 bytes)\n",
            "Collecting beautifulsoup4 (from bs4)\n",
            "  Downloading beautifulsoup4-4.12.3-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting soupsieve>1.2 (from beautifulsoup4->bs4)\n",
            "  Downloading soupsieve-2.5-py3-none-any.whl.metadata (4.7 kB)\n",
            "Downloading bs4-0.0.2-py2.py3-none-any.whl (1.2 kB)\n",
            "Downloading beautifulsoup4-4.12.3-py3-none-any.whl (147 kB)\n",
            "   ---------------------------------------- 0.0/147.9 kB ? eta -:--:--\n",
            "   -- ------------------------------------- 10.2/147.9 kB ? eta -:--:--\n",
            "   ----- --------------------------------- 20.5/147.9 kB 162.5 kB/s eta 0:00:01\n",
            "   ---------- ---------------------------- 41.0/147.9 kB 279.3 kB/s eta 0:00:01\n",
            "   ---------------- ---------------------- 61.4/147.9 kB 465.5 kB/s eta 0:00:01\n",
            "   ------------------------------- ------ 122.9/147.9 kB 554.9 kB/s eta 0:00:01\n",
            "   -------------------------------------- 147.9/147.9 kB 585.9 kB/s eta 0:00:00\n",
            "Downloading soupsieve-2.5-py3-none-any.whl (36 kB)\n",
            "Installing collected packages: soupsieve, beautifulsoup4, bs4\n",
            "Successfully installed beautifulsoup4-4.12.3 bs4-0.0.2 soupsieve-2.5\n"
          ]
        }
      ],
      "source": [
        "!pip install bs4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gZfeob_wzBm-",
        "outputId": "355ee5fe-621a-4e88-d825-3235e787201d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Resume\n",
            "\n",
            "\n",
            "Mark Zuckerburg\n",
            "\n",
            "\n",
            "\n",
            "Summary\n",
            "\n",
            "\n",
            "Mark Zuckerberg, in full Mark Elliot Zuckerberg, (born May 14, 1984, White Plains, New York, U.S.), American computer programmer who was cofounder and CEO (2004– ) of Facebook, a social networking Web site.\n",
            "\n",
            "                After attending Phillips Exeter Academy, Zuckerberg enrolled at Harvard University in 2002. On February 4, 2004, he launched thefacebook.com (renamed Facebook in 2005), a directory in which fellow Harvard students entered their own information and photos into a template that he had devised. Within two weeks half of the student body had signed up. Zuckerberg’s roommates, Dustin Moskovitz and Chris Hughes, helped him add features and make the site available to other campuses across the country. Facebook quickly became popular as registered users could create profiles, upload photos and other media, and keep in touch with friends. It differed from other social networking sites, however, in its emphasis on real names (and e-mail addresses), or “trusted connections.” It also laid particular emphasis on networking, with information disseminated not only to each individual’s network of friends but also to friends of friends—what Zuckerberg called the “social graph.”\n",
            "\n",
            "\n",
            "Skills\n",
            "\n",
            "\n",
            "My skills include :\n",
            "Coding\n",
            "HTML\n",
            "illegally getting data\n",
            "\n",
            "\n",
            "Schooling\n",
            "\n",
            "Zuckerberg attended Harvard University, where he launched Facebook in February 2004 with his roommates Eduardo Saverin, Andrew McCollum, Dustin Moskovitz, and Chris Hughes. Originally launched to select college campuses, the site expanded rapidly and eventually beyond colleges, reaching one billion users by 2012. Zuckerberg took the company public in May 2012 with majority shares. In 2007, at age 23, he became the world's youngest self-made billionaire. He has used his funds to organize multiple philanthropic endeavors, including the Chan Zuckerberg Initiative.\n",
            "\n",
            "\n",
            "Achievements\n",
            "\n",
            "\n",
            "\n",
            "School Grade\n",
            "A+\n",
            "\n",
            "College CGPA\n",
            "9.4\n",
            "\n",
            "IMO Ranking\n",
            "15\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def extract_text_from_html(html_path):\n",
        "    with open(html_path, 'r', encoding='utf-8') as f:\n",
        "        html_content = f.read()\n",
        "\n",
        "    # Parse HTML content\n",
        "    soup = BeautifulSoup(html_content, 'html.parser')\n",
        "\n",
        "    # Extract text recursively from all elements\n",
        "    def extract_text_recursive(element):\n",
        "        text = ''\n",
        "        if element.name in ['script', 'style', 'head', 'title', 'meta']:\n",
        "            return ''\n",
        "        if isinstance(element, str):\n",
        "            return element.strip() + '\\n'\n",
        "        for child in element.children:\n",
        "            text += extract_text_recursive(child)\n",
        "        return text\n",
        "\n",
        "    text_data = extract_text_recursive(soup)\n",
        "\n",
        "    return text_data\n",
        "\n",
        "text = extract_text_from_html('practical2a.html')\n",
        "print(text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pw1vEq3a0RpZ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
